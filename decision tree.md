# ğŸŒ³ Decision Tree Classifier

A **Decision Tree** is a supervised machine learning algorithm used for **classification** and **regression** tasks. It splits the data into branches to make decisions based on certain features â€” like a **flowchart**, where each node represents a decision rule on a feature, and each **leaf node** represents an outcome (class label). ğŸ§ ğŸ“Š

---

## ğŸ§© Key Concepts in Decision Trees:

- **ğŸŒ± Root Node**: Represents the entire dataset and splits into two or more branches.
- **ğŸ”€ Decision Node**: A node that splits further based on a condition.
- **ğŸƒ Leaf Node**: A terminal node that holds a class label (final decision).
- **ğŸŒ¿ Branches**: Represent the outcome of a decision and connect nodes.
- **âœ‚ï¸ Splitting**: The process of dividing nodes into sub-nodes based on features.

---

## ğŸ“ Criteria for Splitting:

To split the dataset effectively, we use statistical measures such as:

- ğŸ“˜ **Information Gain**
- âš–ï¸ **Gini Index**
- ğŸ“Š **Gain Ratio**

# ğŸ§  ID3 Algorithm â€“ Theoretical Explanation

**ID3 (Iterative Dichotomiser 3)** is an algorithm introduced by Ross Quinlan, used to generate a decision tree. It uses **Information Gain** as the splitting criterion. ğŸŒ³

---

## ğŸ” Steps of the ID3 Algorithm:

### 1ï¸âƒ£ Start at the Root Node:
- Take the **whole training dataset** as input.

### 2ï¸âƒ£ Calculate Entropy for the Dataset:
- Entropy measures the **impurity** or **uncertainty** in the dataset. ğŸ²

#### ğŸ”¢ Formula:

$Entropy(S) = - \sum_{i=1}^{c} p_i \log_2(p_i)$

Where:
- $( p_i )$ is the **proportion** of class ( i ) in set ( S ).
- ( c ) is the number of classes.

---

### 3ï¸âƒ£ Calculate Information Gain for Each Attribute:
- **Information Gain** measures the **reduction in entropy** after the dataset is split on an attribute. ğŸ“‰

#### ğŸ”¢ Formula:

$Gain(S, A) = Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \cdot Entropy(S_v)$

Where:
- \( Values(A) \) are the possible values of attribute \( A \).
- $( S_v )$ is the subset of \( S \) where attribute \( A = v \).

---

### 4ï¸âƒ£ Choose the Attribute with the Highest Information Gain:
- This becomes the **decision node**. ğŸŒ
- Create **branches** for each possible value of the chosen attribute.

---

### 5ï¸âƒ£ Repeat the Process Recursively:
- For each branch (subset of data), **repeat** the process with the **remaining attributes**. ğŸ”„

---

### ğŸ›‘ Stop When:
- âœ… All attributes are used.
- âœ… All data in the subset belong to the **same class**.
- âœ… The dataset is **empty** (in this case, assign the **majority class** of the parent node).

---

ğŸ” The ID3 algorithm is a fundamental building block in understanding how decision trees are constructed and why they work the way they do.

## ğŸ“Œ Summary:

- **Decision Tree Classifier**:  
  A model that makes decisions by **splitting data** into subsets using **conditional rules**.

- **ID3 Algorithm**:  
  Builds decision trees using **Entropy** and **Information Gain** to select the **best attribute** at each step.

---
# Example-01
# ğŸŒ¤ï¸ Weather Dataset

| ğŸ“… Day | ğŸŒ¥ï¸ Outlook  | ğŸŒ¡ï¸ Temperature | ğŸ’§ Humidity | ğŸ’¨ Wind   | ğŸ¾ PlayTennis |
|-------:|:-----------:|:--------------:|:-----------:|:--------:|:-------------:|
| D1     | Sunny       | Hot            | High        | Weak     | No            |
| D2     | Sunny       | Hot            | High        | Strong   | No            |
| D3     | Overcast    | Hot            | High        | Weak     | Yes           |
| D4     | Rain        | Mild           | High        | Weak     | Yes           |
| D5     | Rain        | Cool           | Normal      | Weak     | Yes           |
| D6     | Rain        | Cool           | Normal      | Strong   | No            |
| D7     | Overcast    | Cool           | Normal      | Strong   | Yes           |
| D8     | Sunny       | Mild           | High        | Weak     | No            |
| D9     | Sunny       | Cool           | Normal      | Weak     | Yes           |
| D10    | Rain        | Mild           | Normal      | Weak     | Yes           |
| D11    | Sunny       | Mild           | Normal      | Strong   | Yes           |
| D12    | Overcast    | Mild           | High        | Strong   | Yes           |
| D13    | Overcast    | Hot            | Normal      | Weak     | Yes           |
| D14    | Rain        | Mild           | High        | Strong   | No            |

## ğŸ”¶ Step 1: Calculate Entropy of Target (ğŸ¾ PlayTennis)

We have:
- âœ… "Yes" = 9  
- âŒ "No" = 5  

### ğŸ§® Entropy Formula:
$Entropy(S) = -p_+ \log_2(p_+) - p_- \log_2(p_-)$

Where:
- $( p_+ = \frac{9}{14} \approx 0.643 )$
- $( p_- = \frac{5}{14} \approx 0.357 )$

### ğŸ”¢ Substituting Values:
$Entropy(S) = -0.643 \cdot \log_2(0.643) - 0.357 \cdot \log_2(0.357)$

### ğŸ§  Final Answer:
$Entropy(S) \approx 0.940$

## ğŸ”¶ **Step 2: Calculate Information Gain for Each Attribute**

### ğŸ…°ï¸ Outlook

- **Sunny** â†’ [D1, D2, D8, D9, D11] â†’ 5 samples â†’ âœ… 2 Yes, âŒ 3 No  
- **Overcast** â†’ [D3, D7, D12, D13] â†’ 4 samples â†’ âœ… 4 Yes, âŒ 0 No  
- **Rain** â†’ [D4, D5, D6, D10, D14] â†’ 5 samples â†’ âœ… 3 Yes, âŒ 2 No

---

ğŸ“Œ **Formula:**  
`Gain(S, Outlook) = Entropy(S) - âˆ‘ (|Sáµ¥| / |S|) * Entropy(Sáµ¥)`

---

ğŸ”¸ **Entropy of subsets:**

- **Sunny:**  
  `Entropy = âˆ’(2/5)logâ‚‚(2/5) âˆ’ (3/5)logâ‚‚(3/5) â‰ˆ 0.971`

- **Overcast:**  
  All "Yes" â†’ `Entropy = 0`

- **Rain:**  
  `Entropy = âˆ’(3/5)logâ‚‚(3/5) âˆ’ (2/5)logâ‚‚(2/5) â‰ˆ 0.971`

---

ğŸ”¸ **Information Gain (Outlook):**  
`Gain(Outlook) = 0.940 âˆ’ [(5/14 Ã— 0.971) + (4/14 Ã— 0) + (5/14 Ã— 0.971)]`  
`= 0.940 âˆ’ (0.347 + 0 + 0.347)`  
`= 0.940 âˆ’ 0.694`  
ğŸ¯ `= 0.246`


### ğŸŸ  B. Temperature

- **Hot** â†’ [D1, D2, D3, D13] â†’ 4 samples â†’ âœ… 2 Yes, âŒ 2 No  
  â¤ Entropy = **1.0**

- **Mild** â†’ [D4, D8, D10, D11, D12, D14] â†’ 6 samples â†’ âœ… 4 Yes, âŒ 2 No  
  â¤ Entropy =  
  $-\frac{4}{6} \log_2\left(\frac{4}{6}\right) - \frac{2}{6} \log_2\left(\frac{2}{6}\right) â‰ˆ 0.918$

- **Cool** â†’ [D5, D6, D7, D9] â†’ 4 samples â†’ âœ… 3 Yes, âŒ 1 No  
  â¤ Entropy =  
  $-\frac{3}{4} \log_2\left(\frac{3}{4}\right) - \frac{1}{4} \log_2\left(\frac{1}{4}\right) â‰ˆ 0.811$

---

ğŸ”¸ **Information Gain (Temperature):** 

$\text{Gain(Temperature)} = 0.940 - \left( \frac{4}{14} \cdot 1.0 + \frac{6}{14} \cdot 0.918 + \frac{4}{14} \cdot 0.811 \right)$

$= 0.940 - (0.286 + 0.393 + 0.232)$

$= 0.940 - 0.911 = \boxed{0.029}$

### ğŸŒ«ï¸ C. Humidity

- **High** â†’ [D1, D2, D3, D4, D8, D12, D14] â†’ 7 samples â†’ âœ… 3 Yes, âŒ 4 No  
  â¤ Entropy =  
  $-\frac{3}{7} \log_2\left(\frac{3}{7}\right) - \frac{4}{7} \log_2\left(\frac{4}{7}\right) â‰ˆ 0.985$

- **Normal** â†’ [D5, D6, D7, D9, D10, D11, D13] â†’ 7 samples â†’ âœ… 6 Yes, âŒ 1 No  
  â¤ Entropy =  
  $-\frac{6}{7} \log_2\left(\frac{6}{7}\right) - \frac{1}{7} \log_2\left(\frac{1}{7}\right) â‰ˆ 0.591$

---

ğŸ”¸ **Information Gain (Humidity):** 

$\text{Gain(Humidity)} = 0.940 - \left( \frac{7}{14} \cdot 0.985 + \frac{7}{14} \cdot 0.591 \right)$

$= 0.940 - (0.492 + 0.296)$

$= 0.940 - 0.788 = \boxed{0.152}$

### ğŸŒ¬ï¸ D. Wind

- **Weak** â†’ [D1, D3, D4, D5, D8, D9, D10] â†’ 7 samples â†’ âœ… 6 Yes, âŒ 1 No  
  â¤ Entropy =  
  $-\frac{6}{7} \log_2\left(\frac{6}{7}\right) - \frac{1}{7} \log_2\left(\frac{1}{7}\right) â‰ˆ 0.591$

- **Strong** â†’ [D2, D6, D7, D11, D12, D13, D14] â†’ 7 samples â†’ âœ… 3 Yes, âŒ 4 No  
  â¤ Entropy =  
  $-\frac{3}{7} \log_2\left(\frac{3}{7}\right) - \frac{4}{7} \log_2\left(\frac{4}{7}\right) â‰ˆ 0.985$

---

ğŸ”¸ **Information Gain (Wind):** 

$\text{Gain(Wind)} = 0.940 - \left( \frac{7}{14} \cdot 0.591 + \frac{7}{14} \cdot 0.985 \right)$

$= 0.940 - (0.296 + 0.492)$

$= 0.940 - 0.788 = \boxed{0.152}$

### ğŸ”¶ **Best Attribute to Split On:**

âœ… **Outlook** has the highest information gain of **0.246**, so it is the best attribute to split on.

---
ğŸ“Š **Decision:**
The first node of the decision tree will be based on the **Outlook** attribute, as it provides the most useful split in terms of Information Gain.



