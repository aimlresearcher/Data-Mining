# ðŸ§  Naive Bayes Sentiment Analysis â€” Step-by-Step with Preprocessing (Imbalanced Dataset)

---

## âœ… Outline

We'll walk through the following steps:

1. **Dataset Setup (Imbalanced)**
2. **Text Preprocessing**
   - Lowercasing
   - Removing punctuation
   - Removing stopwords
   - Tokenization
3. **Vocabulary Creation**
4. **Word Frequency Calculation per Class**
5. **Calculate Prior Probabilities**
6. **Apply Naive Bayes Formula**
7. **Make Prediction**

---

## 1. ðŸ“Š Dataset Setup (Imbalanced)

| Sentence                                 | Sentiment |
|------------------------------------------|-----------|
| I loved the visuals and the direction    | Positive  |
| The movie was heartwarming and sweet     | Positive  |
| A wonderful and delightful experience    | Positive  |
| It had charming characters and great music | Positive |
| The story was dull and boring            | Negative  |
| I hated the plot and the acting          | Negative  |

âœ… Total: 6 sentences  
- Positive = 4  
- Negative = 2

---

## 2. ðŸ§¹ Text Preprocessing

### ðŸ”§ Preprocessing Includes:
- Convert to **lowercase**
- **Remove punctuation**
- Remove **stopwords**
- Tokenize into **words**

### ðŸ›‘ Stopword List:
`["i", "the", "and", "was", "it", "had"]`

---

### âœ… Preprocessed Training Sentences

#### Positive:
1. `loved visuals direction`  
2. `movie heartwarming sweet`  
3. `wonderful delightful experience`  
4. `charming characters great music`

â†’ Positive Words =  
`["loved", "visuals", "direction", "movie", "heartwarming", "sweet", "wonderful", "delightful", "experience", "charming", "characters", "great", "music"]`  
ðŸ”¢ Total = 13 words

#### Negative:
1. `story dull boring`  
2. `hated plot acting`

â†’ Negative Words =  
`["story", "dull", "boring", "hated", "plot", "acting"]`  
ðŸ”¢ Total = 6 words

---

### ðŸŽ¯ Test Sentence

**Original:** `"I loved the movie but the story was boring"`  
**Preprocessed:** `["loved", "movie", "story", "boring"]`

---

## 3. ðŸ“š Vocabulary

All unique words:

```
["loved", "visuals", "direction", "movie", "heartwarming", "sweet", "wonderful", "delightful", 
 "experience", "charming", "characters", "great", "music", "story", "dull", "boring", "hated", 
 "plot", "acting"]
```

ðŸ“¦ Vocabulary size = **19**

---

## 4. ðŸ”¢ Word Frequencies

| Word        | Positive Count | Negative Count |
|-------------|----------------|----------------|
| loved       | 1              | 0              |
| visuals     | 1              | 0              |
| direction   | 1              | 0              |
| movie       | 1              | 0              |
| heartwarming| 1              | 0              |
| sweet       | 1              | 0              |
| wonderful   | 1              | 0              |
| delightful  | 1              | 0              |
| experience  | 1              | 0              |
| charming    | 1              | 0              |
| characters  | 1              | 0              |
| great       | 1              | 0              |
| music       | 1              | 0              |
| story       | 0              | 1              |
| dull        | 0              | 1              |
| boring      | 0              | 1              |
| hated       | 0              | 1              |
| plot        | 0              | 1              |
| acting      | 0              | 1              |

---

## 5. ðŸ“ Prior Probabilities

- P(Positive) = 4/6 = **0.6667**
- P(Negative) = 2/6 = **0.3333**

---

## 6. ðŸ“ Apply Naive Bayes Formula (with Laplace Smoothing)

Formula:

```
P(w|class) = (count(w) + 1) / (total_words_in_class + |V|)
```

---

### ðŸ”¹ Positive Class

- Total words = 13  
- Vocabulary size = 19  
- Denominator = 13 + 19 = 32

Test words: `["loved", "movie", "story", "boring"]`

| Word    | Count | P(w\|Positive) |
|---------|--------|---------------|
| loved   | 1      | 2/32  
| movie   | 1      | 2/32  
| story   | 0      | 1/32  
| boring  | 0      | 1/32  

```
P(X|Positive) = (2/32) Ã— (2/32) Ã— (1/32) Ã— (1/32) = 4 / 1048576 â‰ˆ 3.81 Ã— 10â»â¶
P(Positive | X) = P(X | Positive) Ã— P(Positive)  
                = (2/32 Ã— 2/32 Ã— 1/32 Ã— 1/32) Ã— 0.6667  
                = (4 / 1048576) Ã— 0.6667  
                â‰ˆ 3.81 Ã— 10â»â¶ Ã— 0.6667  
                â‰ˆ 2.54 Ã— 10â»â¶
```

---

### ðŸ”» Negative Class

- Total words = 6  
- Denominator = 6 + 19 = 25

Test words: `["loved", "movie", "story", "boring"]`

| Word    | Count | P(w|Negative) |
|---------|--------|---------------|
| loved   | 0      | 1/25  
| movie   | 0      | 1/25  
| story   | 1      | 2/25  
| boring  | 1      | 2/25  

```
P(X|Negative) = (1/25) Ã— (1/25) Ã— (2/25) Ã— (2/25) = 4 / 390625 â‰ˆ 1.02 Ã— 10â»âµ
P(Negative | X) = P(X | Negative) Ã— P(Negative)  
                = (4 / 390625) Ã— 0.3333  
                â‰ˆ 1.02 Ã— 10â»âµ Ã— 0.3333  
                â‰ˆ 3.41 Ã— 10â»â¶

```

---

## âœ… 7. Final Prediction

| Sentiment | Posterior Probability |
|-----------|------------------------|
| Positive  | â‰ˆ 2.54 Ã— 10â»â¶          |
| Negative  | â‰ˆ 3.41 Ã— 10â»â¶ âœ…        |

### âœ… Final Result: **Negative**

Even though the dataset had more positive sentences, the test sentence had strong negative indicators like â€œstoryâ€ and â€œboringâ€ which were **never seen in positive class** â€” so Naive Bayes leaned negative.

---

## ðŸ“ Summary Table

| Step              | Action                                               |
|-------------------|-----------------------------------------------------|
| Preprocessing     | Lowercase, remove punctuation/stopwords, tokenize   |
| Vocabulary        | Extracted from training set                         |
| Frequencies       | Count words in each class                           |
| Priors            | Based on class distribution                         |
| Naive Bayes       | With Laplace smoothing                              |
| Prediction        | Posterior probabilities determine final class       |

---



# â“ Why Use Laplace Smoothing Instead of Just Bayesâ€™ Theorem?

---

## ðŸ“˜ What Is Bayesâ€™ Theorem?

Bayes' Theorem is the **core formula** used in Naive Bayes classification:

$
P(C_k \mid X) = \frac{P(X \mid C_k) \cdot P(C_k)}{P(X)}
$

Where:
- \( P(C_k \mid X) \) is the **posterior** probability of class \( C_k \) given input \( X \)
- \( P(X \mid C_k) \) is the **likelihood** of observing \( X \) given class \( C_k \)
- \( P(C_k) \) is the **prior** probability of class \( C_k \)
- \( P(X) \) is the probability of input \( X \) (same across all classes)

---

## ðŸ§  How Naive Bayes Uses It

In text classification:

$
P(X \mid C_k) = P(w_1 \mid C_k) \cdot P(w_2 \mid C_k) \cdot \ldots \cdot P(w_n \mid C_k)
$

We multiply the probabilities of all words \( w_1, w_2, \dots, w_n \) in the sentence.

---

## âŒ The Problem: Zero Probability

Letâ€™s say the test sentence contains a word that **never appeared** in class \( C_k \):

Example:
- Word **"awesome"** appears in a test sentence
- It never occurred in **Negative** training examples

Then:
$
P(\text{"awesome"} \mid \text{Negative}) = 0
$

So:
$
P(X \mid \text{Negative}) = 0
\Rightarrow P(\text{Negative} \mid X) = 0
$

**Even if all other words strongly support the Negative class, the entire probability becomes zero!**

---

## âœ… The Solution: Laplace Smoothing

To fix this, we apply **Laplace Smoothing** (also called **Add-One Smoothing**):

$
P(w_i \mid C_k) = \frac{\text{count}(w_i \text{ in class}) + 1}{\text{total words in class} + |V|}
$

Where:
- \( |V| \) is the **vocabulary size** (total unique words across all classes)

This means:
- Every word has **at least a small non-zero probability**
- Even **unseen words** are assigned a small probability

---

## ðŸ“ Summary Table

| Concept                 | Explanation                                                                 |
|-------------------------|-----------------------------------------------------------------------------|
| Bayesâ€™ Theorem          | Calculates posterior using prior and likelihood                            |
| Why smoothing?          | Avoids multiplying by zero when a word doesnâ€™t exist in training data       |
| Laplace Smoothing       | Adds 1 to each word count so unseen words still have small probability      |
| Impact                  | Makes Naive Bayes **robust** to new/unseen words in test data               |

---

## ðŸ’¬ Teaching Tip

You can demonstrate this with a small example:

- Train one class with: `["happy", "smile"]`
- Test with: `["happy", "awesome"]`

Without smoothing â†’ result = 0  
With smoothing â†’ result > 0

